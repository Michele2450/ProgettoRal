{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "DLFUl1bqyp-A"
      },
      "outputs": [],
      "source": [
        "# Standard\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "# Third-party\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "import gc\n",
        "# knister api\n",
        "from api import KnisterGame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Tui5TWb0s_tw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sjLdnOl3s-oh"
      },
      "outputs": [],
      "source": [
        "class QNet(nn.Module):\n",
        "    # Policy Network\n",
        "    def __init__(self, n_state_vars, n_actions, dim_hidden=256):\n",
        "        super(QNet, self).__init__()\n",
        "\n",
        "        # Define a feedforward neural network with hidden layers, ReLU\n",
        "        #  activations, and an output layer that maps to the number of actionsh\n",
        "        # creo una rete ad imbuto in modo che il modello posso filtrare le azioni meno rilevanti per la stima dei Q-values\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_state_vars, dim_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_hidden, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passes the input through the network layers to output Q-values\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "xvVh-ABA8g-q"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, n_actions, memory_size, batch_size):\n",
        "        # Initialize actions, batch and experience template\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        # Initialize the memory\n",
        "        self.memory = deque(maxlen=memory_size)  # Hint: use deque\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Store experience in memory\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        # Sample a batch of experiences\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)  # Hint: use random\n",
        "\n",
        "        # Convert to tensors for training\n",
        "        states = torch.from_numpy(\n",
        "            np.vstack([e.state for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        actions = torch.from_numpy(\n",
        "            np.vstack([e.action for e in experiences if e is not None])\n",
        "        ).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(\n",
        "            np.vstack([e.reward for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        next_states = torch.from_numpy(\n",
        "            np.vstack([e.next_state for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        dones = torch.from_numpy(\n",
        "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n",
        "        ).float().to(device)\n",
        "\n",
        "        # Return the tuple with all tensors\n",
        "        return (states,actions,rewards,next_states,dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "DdlsVjtx27Wt"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(\n",
        "        self, n_states, n_actions, batch_size=64, learning_rate=1e-4,\n",
        "        learn_step=5, gamma=0.99, mem_size=int(1e5), tau=1e-3\n",
        "    ):\n",
        "        # Core parameters for learning and updating the Q-network\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.learn_step = learn_step  # Frequency of learning steps\n",
        "        self.tau = tau  # Rate for soft updating the target network\n",
        "\n",
        "        # Initialize the policy network (net_eval) and target network (net_target)\n",
        "        self.net_eval = QNet(n_states,n_actions).to(device)  # Hint: we have a class for this\n",
        "        self.net_target = QNet(n_states,n_actions).to(device)  # Hint: we have a class for this\n",
        "        self.optimizer = optim.Adam(self.net_eval.parameters(),lr=learning_rate)  # Suggestion: use Adam from `optim` with specific learning rate\n",
        "        self.criterion =  nn.MSELoss() # Suggestion: use Mean Squared Error (MSE) as the loss function\n",
        "\n",
        "        # Initialize memory for experience replay\n",
        "        self.memory = ReplayBuffer(n_actions,mem_size,batch_size)  # Hint: we have a class for this\n",
        "        self.counter = 0  # Tracks learning steps for periodic updates\n",
        "\n",
        "    def getAction(self, state,available_actions, epsilon):\n",
        "        # Select action using an epsilon-greedy strategy to balance exploration\n",
        "        #  and exploitation\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.net_eval.eval()  # Set network to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            action_values = self.net_eval(state)\n",
        "        self.net_eval.train()  # Return to training mode\n",
        "\n",
        "        # Choose random action with probability epsilon, otherwise choose best\n",
        "        #  predicted action\n",
        "        if random.random() < epsilon:\n",
        "            #action = random.choice(np.arange(self.n_actions))\n",
        "            action = random.choice(available_actions)\n",
        "        else:\n",
        "            # Calcolo dei q_values\n",
        "            q_values = action_values.cpu().data.numpy().squeeze()\n",
        "            # Creiamo una maschera con -infinito\n",
        "            mask = np.full(q_values.shape, -1e5)\n",
        "            # Copiamo solo i valori delle azioni VALIDE nella maschera\n",
        "            mask[available_actions] = q_values[available_actions]\n",
        "            # Argmax sulla maschera sceglierà sempre un'azione valida\n",
        "            action = np.argmax(mask)\n",
        "            #action =np.argmax(action_values.cpu().data.numpy())   # Hint: you may find `np.argmax` useful\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def save2Memory(self, state, action, reward, next_state, done):\n",
        "        # Save experience to memory and, if ready, sample from memory and\n",
        "        #  update the network\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.counter += 1  # Increment step counter\n",
        "\n",
        "        # Perform learning every 'learn_step' steps if enough experiences are\n",
        "        #  in memory\n",
        "        if (self.counter % self.learn_step == 0 and len(self.memory) >= self.batch_size):  # Hint: check if counter is a multiple of learn_step and  memory has enough samples\n",
        "            experiences= self.memory.sample() # Hint: retrieve a sample of experiences\n",
        "            self.learn(experiences)\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        # Perform a learning step by minimizing the difference between\n",
        "        #  predicted and target Q-values\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        \n",
        "        ## AGGIUNTA DELLA MASCHERA PER LE AZIONI NON VALIDE ##\n",
        "        q_target_predictions = self.net_target(next_states).detach()\n",
        "        # recupero le griglie degli stati successivi\n",
        "        next_states_grids = next_states[:, :25]\n",
        "        # Creo la maschera per le azioni valide\n",
        "        invalid_mask = (next_states_grids != 0)\n",
        "        # Imposto a -inf i Q-values delle azioni non valide\n",
        "        q_target_predictions[invalid_mask] = -1e5\n",
        "\n",
        "        # Compute target Q-values from net_target for stability in training\n",
        "        q_target = q_target_predictions.max(1)[0].unsqueeze(1)\n",
        "        \"\"\"# Compute target Q-values from net_target for stability in training\n",
        "        q_target = self.net_target(next_states).detach().max(1)[0].unsqueeze(1)\"\"\"\n",
        "        y_j = rewards + (self.gamma * q_target * (1 - dones))\n",
        "            # Bellman equation for target Q-value\n",
        "        q_eval = self.net_eval(states).gather(1, actions)\n",
        "            # Q-value predictions from policy network\n",
        "\n",
        "        # Compute loss and backpropagate to update net_eval\n",
        "        loss = self.criterion(q_eval, y_j)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network with soft update for smooth learning\n",
        "        self.targetUpdate()\n",
        "\n",
        "    def targetUpdate(self):\n",
        "        # Soft update to gradually shift target network parameters toward\n",
        "        #  policy network parameters\n",
        "        params = zip(self.net_eval.parameters(), self.net_target.parameters())\n",
        "        for eval_param, target_param in params:\n",
        "            target_param.data.copy_(\n",
        "                self.tau  * eval_param.data + (1.0-self.tau) * target_param.data # Hint: use the `self.tau` rate for soft updating\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Funzione di calcolo dello stato dell'ambiente e one hot encoding del dado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_obs_state(game):\n",
        "    \"\"\"\n",
        "    Converte lo stato:\n",
        "    - Griglia: 25 valori normalizzati (0-1)\n",
        "    - Dado: One-Hot Encoding (vettore di 11 zeri con un 1)\n",
        "    \"\"\"\n",
        "    # 1. Griglia Normalizzata (25 valori)\n",
        "    grid = game.get_grid().flatten() / 12.0\n",
        "    \n",
        "    # 2. Dado One-Hot Encoding (11 valori)\n",
        "    # I dadi vanno da 2 a 12 (11 possibili risultati)\n",
        "    dice_val = int(game.get_current_roll())\n",
        "    \n",
        "    # Creo un vettore di 11 zeri\n",
        "    dice_one_hot = np.zeros(11, dtype=np.float32)\n",
        "    \n",
        "    # Metto '1' nella posizione giusta\n",
        "    # Se esce 2 -> indice 0. Se esce 12 -> indice 10.\n",
        "    if 2 <= dice_val <= 12:\n",
        "        dice_one_hot[dice_val - 2] = 1.0\n",
        "        \n",
        "    # Unisco tutto: 25 (griglia) + 11 (dado) = 36 valori totali\n",
        "    obs = np.concatenate([grid, dice_one_hot])\n",
        "    \n",
        "    return obs.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "lhRfNjag7fgy"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_NAME = 'checkpoint.pth'\n",
        "RECENT_EPISODES = 100  # Number of episodes for average score in early stopping\n",
        "MIN_EPISODES_FOR_STOP = 100  # Ensures enough episodes before evaluating target\n",
        "\n",
        "def train(\n",
        "        env, agent, n_episodes, max_steps,\n",
        "        eps_start, eps_end, eps_decay,\n",
        "        target_score, do_store_checkpoint\n",
        "):\n",
        "    # Initialize score history and epsilon (exploration rate)\n",
        "    game_score_hist = []\n",
        "    reward_hist = []\n",
        "    epsilon_hist = []\n",
        "    epsilon = eps_start\n",
        "\n",
        "    # Progress bar format for tracking training progress\n",
        "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt}'\\\n",
        "                 ' [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
        "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
        "\n",
        "    for idx_epi in pbar:\n",
        "        # Reset the environment for a new episode\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)\n",
        "        score = 0.0\n",
        "\n",
        "        for idx_step in range(max_steps):\n",
        "            # Select an action based on the current policy (epsilon-greedy)\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon)  # Hint: we have something ready for this\n",
        "\n",
        "            #reward_shaping = calculate_potential(env.get_grid())\n",
        "            \n",
        "            # Execute the chosen action in the environment            \n",
        "            env.choose_action(action)\n",
        "\n",
        "            #cambio di stato\n",
        "            next_state = get_obs_state(env)\n",
        "            \n",
        "            #creo la reward totale sommando il reward shaping\n",
        "            reward = env.get_last_reward() \n",
        "            \n",
        "            # Check if the episode is finished\n",
        "            done = env.has_finished()\n",
        "            \n",
        "            # Store experience in memory and update the agent\n",
        "            agent.save2Memory(state,action,reward,next_state,done)  # Hint: what data do we store as \"experience\"? How we do it?\n",
        "            state = next_state  # Move to the next state\n",
        "            \n",
        "            score += reward  # Hint: what's our total cumulative score?\n",
        "            # Check if the episode is finished\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Track scores and decay epsilon for less exploration over time\n",
        "        #salva lo score di ogni episodio per vederlo nel grafico\n",
        "        game_score = env.get_total_reward()\n",
        "        game_score_hist.append(game_score)\n",
        "        epsilon_hist.append(epsilon)\n",
        "        score_avg = np.mean(game_score_hist[-RECENT_EPISODES:])\n",
        "        epsilon =max(epsilon - eps_decay,eps_end)  # Hint: epsilon decreases, but we have a minimum \n",
        "        \"\"\"\"per la epsiolon\"\"\"\n",
        "\n",
        "        # Update the progress bar with the current score and average\n",
        "        pbar.set_postfix_str(\n",
        "            f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}, Eps: {epsilon: .4f}\"\n",
        "        )\n",
        "        pbar.update(0)\n",
        "\n",
        "        # Clear GPU memory periodically to prevent memory issues\n",
        "        if idx_epi % 1000 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "        # Early stopping condition if target score is achieved\n",
        "        if len(game_score_hist) >= 100 and score_avg >= target_score:\n",
        "            print(\"\\nTarget Reached!\")\n",
        "            break\n",
        "\n",
        "    # Print completion message based on early stopping or max episodes\n",
        "    if (idx_epi + 1) < n_episodes:\n",
        "        print(\"\\nTraining complete - target reached!\")\n",
        "    else:\n",
        "        print(\"\\nTraining complete - maximum episodes reached.\")\n",
        "\n",
        "    # Save the trained model if specified\n",
        "    if do_store_checkpoint:\n",
        "        torch.save(agent.net_eval.state_dict(), CHECKPOINT_NAME)\n",
        "\n",
        "    return game_score_hist, epsilon_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotReward(rewards):\n",
        "    # Plot the agent's reward history to visualize learning progress\n",
        "    plt.figure()\n",
        "    plt.plot(rewards)\n",
        "    plt.title(\"Reward History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Reward\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "lw3c6cHKzInm"
      },
      "outputs": [],
      "source": [
        "def plotScore(scores):\n",
        "    # Plot the agent's score history to visualize learning progress\n",
        "    plt.figure()\n",
        "    plt.plot(scores)\n",
        "    plt.title(\"Score History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotEpsilon(epsilons):\n",
        "    # Plot the agent's epsilon history to visualize exploration rate changes\n",
        "    plt.figure()\n",
        "    plt.plot(epsilons)\n",
        "    plt.title(\"Epsilon History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotAll(scores, epsilons):\n",
        "    # Plot reward, score, and epsilon histories in a single figure with subplots\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(scores)\n",
        "    plt.title(\"Score History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epsilons)\n",
        "    plt.title(\"Epsilon History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vCUG7M0nWW"
      },
      "source": [
        "## **Section 4 - Time to learn (training)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "tm7eqwwA0n9M"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64         # Number of experiences sampled per learning step\n",
        "LR = 1e-3                # Learning rate for optimizer\n",
        "EPISODES = 200000        # Maximum number of episodes to train (~1 or 2 minute for 100 episodes)\n",
        "TARGET_SCORE = 100        # Early stop if average score reaches this value\n",
        "GAMMA = 0.99             # Discount factor for future rewards\n",
        "MEMORY_SIZE = 100000      # Maximum capacity of replay memory\n",
        "LEARN_STEP = 10          # Frequency (in steps) of learning updates\n",
        "TAU = 1e-3               # Soft update rate for the target network\n",
        "SAVE_CHKPT = True       # Option to save trained model checkpoint\n",
        "#Exploration parameters\n",
        "MAX_STEPS = 25           # Maximum steps per episode\n",
        "EPS_START = 1             # Initial epsilon for exploration (100% exploration at start)\n",
        "EPS_END = 0.05            # Minimum epsilon (final exploration rate)\n",
        "# Epsilon decay rate (controls exploration reduction)\n",
        "EPS_DECAY = (EPS_START - EPS_END) / 50000 # così dopo circa 120000 episodi arriva a EPS_END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWfHJf-A4Gqx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|##########| 200000/200000 [1:14:54<  00:00, 44.50ep/s, Score:   44.00, 100 score avg:   37.06, Eps:  0.0500]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training complete - maximum episodes reached.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 3, got 2)",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[39], line 23\u001b[0m\n\u001b[0;32m     11\u001b[0m agent \u001b[38;5;241m=\u001b[39m DQN(\n\u001b[0;32m     12\u001b[0m     n_states\u001b[38;5;241m=\u001b[39mn_states,\n\u001b[0;32m     13\u001b[0m     n_actions\u001b[38;5;241m=\u001b[39mn_actions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     19\u001b[0m     tau\u001b[38;5;241m=\u001b[39mTAU\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# 4. Avvia il Training (passando il decadimento lineare corretto)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m game_score_hist, reward_hist,epsilon_hist \u001b[38;5;241m=\u001b[39m train(env, agent, n_episodes\u001b[38;5;241m=\u001b[39mEPISODES, max_steps\u001b[38;5;241m=\u001b[39mMAX_STEPS,\n\u001b[0;32m     24\u001b[0m                    eps_start\u001b[38;5;241m=\u001b[39mEPS_START, eps_end\u001b[38;5;241m=\u001b[39mEPS_END, eps_decay\u001b[38;5;241m=\u001b[39mEPS_DECAY,\n\u001b[0;32m     25\u001b[0m                    target_score\u001b[38;5;241m=\u001b[39mTARGET_SCORE, do_store_checkpoint\u001b[38;5;241m=\u001b[39mSAVE_CHKPT)\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# 5. Plotta i risultati\u001b[39;00m\n\u001b[0;32m     27\u001b[0m plotAll(game_score_hist, reward_hist, epsilon_hist)\n",
            "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
          ]
        }
      ],
      "source": [
        "# 1. Crea l'ambiente\n",
        "env = KnisterGame()\n",
        "\n",
        "# 2. Definisci dimensioni stato e azione\n",
        "# Input: 25 caselle + 11 valori dado (one-hot) = 36\n",
        "n_states = 25+11 \n",
        "# Output: 25 possibili posizioni dove scrivere\n",
        "n_actions = 25 \n",
        "\n",
        "# 3. Inizializza l'Agente\n",
        "agent = DQN(\n",
        "    n_states=n_states,\n",
        "    n_actions=n_actions,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR, \n",
        "    mem_size=MEMORY_SIZE,\n",
        "    gamma=GAMMA,\n",
        "    learn_step=LEARN_STEP,\n",
        "    tau=TAU\n",
        ")\n",
        "\n",
        "# 4. Avvia il Training (passando il decadimento lineare corretto)\n",
        "game_score_hist, reward_hist,epsilon_hist = train(env, agent, n_episodes=EPISODES, max_steps=MAX_STEPS,\n",
        "                   eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY,\n",
        "                   target_score=TARGET_SCORE, do_store_checkpoint=SAVE_CHKPT)\n",
        "# 5. Plotta i risultati\n",
        "plotAll(game_score_hist, epsilon_hist)\n",
        "# Free up GPU memory if using CUDA\n",
        "if str(device) == \"cuda\":\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TRAINING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test veloce per vedere quanto fa di media il gioco"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_knister_agent_fast(env, agent,num_tests, max_steps):\n",
        "    print(\"--- INIZIO PARTITA DI TEST ---\")\n",
        "    state = get_obs_state(env)\n",
        "    step = 1\n",
        "    average_score = []\n",
        "    for i in range(num_tests):\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)  \n",
        "        for step in range(max_steps):#episodio\n",
        "            # Nessuna mossa casuale usa solo quello che ha imparato\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon=0.0)\n",
        "            # Esegue la mossa\n",
        "            env.choose_action(action)\n",
        "            #cambio di stato\n",
        "            state = get_obs_state(env)\n",
        "            # controllo fine partita\n",
        "            done = env.has_finished()\n",
        "            step += 1\n",
        "            if done:\n",
        "                break    \n",
        "        average_score.append(env.get_total_reward())\n",
        "        #print(f\"Punteggio Finale: {env.game.get_total_reward()}\")\n",
        "    print(f\"Punteggio Medio: {sum(average_score) / len(average_score) if average_score else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- INIZIO PARTITA DI TEST ---\n",
            "Punteggio Medio: 39.982\n"
          ]
        }
      ],
      "source": [
        "NUM_TEST_EPISODES = 500   # Number of episodes to test the agent\n",
        "MAX_STEPS_TEST = 25    # Maximum steps per test episode\n",
        "if SAVE_CHKPT:\n",
        "    agent.net_eval.load_state_dict(torch.load(CHECKPOINT_NAME))\n",
        "test_knister_agent_fast(env, agent,NUM_TEST_EPISODES, MAX_STEPS_TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test con passi per vedere cosa fa il modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_knister_agent(env, agent,num_tests, max_steps):\n",
        "    print(\"--- INIZIO PARTITA DI TEST ---\")\n",
        "    state = get_obs_state(env)\n",
        "    step = 1\n",
        "    average_score = []\n",
        "    for i in range(num_tests):\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)  \n",
        "        for step in range(max_steps):#episodio\n",
        "            # Nessuna mossa casuale usa solo quello che ha imparato\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon=0.0)\n",
        "            # Esegue la mossa\n",
        "            env.choose_action(action)\n",
        "            #cambio di stato\n",
        "            state = get_obs_state(env)\n",
        "            # controllo fine partita\n",
        "            done = env.has_finished()\n",
        "            \n",
        "            # Recuperiamo info dal gioco reale per visualizzare\n",
        "            current_grid = env.get_grid()\n",
        "            dice_value = env.get_current_roll()\n",
        "            score = env.get_total_reward()\n",
        "            \n",
        "            print(f\"\\nStep {step}\")\n",
        "            print(f\"Dado uscito: {dice_value}\")\n",
        "            print(f\"L'agente ha scelto la posizione: {action} (Riga {action//5}, Col {action%5})\")\n",
        "            print(\"Griglia attuale:\")\n",
        "            print(current_grid)\n",
        "            print(f\"Punteggio attuale: {score}\")\n",
        "            \n",
        "            step += 1\n",
        "            if done:\n",
        "                break    \n",
        "        average_score.append(env.get_total_reward())\n",
        "        #print(f\"Punteggio Finale: {env.game.get_total_reward()}\")\n",
        "    print(f\"Punteggio Medio: {sum(average_score) / len(average_score) if average_score else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- INIZIO PARTITA DI TEST ---\n",
            "\n",
            "Step 0\n",
            "Dado uscito: 2\n",
            "L'agente ha scelto la posizione: 12 (Riga 2, Col 2)\n",
            "Griglia attuale:\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "Punteggio attuale: 0\n",
            "\n",
            "Step 1\n",
            "Dado uscito: 8\n",
            "L'agente ha scelto la posizione: 21 (Riga 4, Col 1)\n",
            "Griglia attuale:\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 2 0 0 0]]\n",
            "Punteggio attuale: 0\n",
            "\n",
            "Step 2\n",
            "Dado uscito: 3\n",
            "L'agente ha scelto la posizione: 8 (Riga 1, Col 3)\n",
            "Griglia attuale:\n",
            "[[0 0 0 0 0]\n",
            " [0 0 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 2 0 0 0]]\n",
            "Punteggio attuale: 0\n",
            "\n",
            "Step 3\n",
            "Dado uscito: 8\n",
            "L'agente ha scelto la posizione: 3 (Riga 0, Col 3)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [0 0 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 2 0 0 0]]\n",
            "Punteggio attuale: 0\n",
            "\n",
            "Step 4\n",
            "Dado uscito: 6\n",
            "L'agente ha scelto la posizione: 20 (Riga 4, Col 0)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [0 0 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [8 2 0 0 0]]\n",
            "Punteggio attuale: 2\n",
            "\n",
            "Step 5\n",
            "Dado uscito: 3\n",
            "L'agente ha scelto la posizione: 6 (Riga 1, Col 1)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [0 6 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [8 2 0 0 0]]\n",
            "Punteggio attuale: 2\n",
            "\n",
            "Step 6\n",
            "Dado uscito: 8\n",
            "L'agente ha scelto la posizione: 22 (Riga 4, Col 2)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [0 6 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [8 2 3 0 0]]\n",
            "Punteggio attuale: 2\n",
            "\n",
            "Step 7\n",
            "Dado uscito: 4\n",
            "L'agente ha scelto la posizione: 5 (Riga 1, Col 0)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [8 6 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [8 2 3 0 0]]\n",
            "Punteggio attuale: 4\n",
            "\n",
            "Step 8\n",
            "Dado uscito: 10\n",
            "L'agente ha scelto la posizione: 17 (Riga 3, Col 2)\n",
            "Griglia attuale:\n",
            "[[0 0 0 3 0]\n",
            " [8 6 0 8 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 4 0 0]\n",
            " [8 2 3 0 0]]\n",
            "Punteggio attuale: 4\n",
            "\n",
            "Step 9\n",
            "Dado uscito: 7\n",
            "L'agente ha scelto la posizione: 13 (Riga 2, Col 3)\n",
            "Griglia attuale:\n",
            "[[ 0  0  0  3  0]\n",
            " [ 8  6  0  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 0  0  4  0  0]\n",
            " [ 8  2  3  0  0]]\n",
            "Punteggio attuale: 4\n",
            "\n",
            "Step 10\n",
            "Dado uscito: 5\n",
            "L'agente ha scelto la posizione: 4 (Riga 0, Col 4)\n",
            "Griglia attuale:\n",
            "[[ 0  0  0  3  7]\n",
            " [ 8  6  0  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 0  0  4  0  0]\n",
            " [ 8  2  3  0  0]]\n",
            "Punteggio attuale: 8\n",
            "\n",
            "Step 11\n",
            "Dado uscito: 7\n",
            "L'agente ha scelto la posizione: 7 (Riga 1, Col 2)\n",
            "Griglia attuale:\n",
            "[[ 0  0  0  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 0  0  4  0  0]\n",
            " [ 8  2  3  0  0]]\n",
            "Punteggio attuale: 8\n",
            "\n",
            "Step 12\n",
            "Dado uscito: 5\n",
            "L'agente ha scelto la posizione: 0 (Riga 0, Col 0)\n",
            "Griglia attuale:\n",
            "[[ 7  0  0  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 0  0  4  0  0]\n",
            " [ 8  2  3  0  0]]\n",
            "Punteggio attuale: 11\n",
            "\n",
            "Step 13\n",
            "Dado uscito: 5\n",
            "L'agente ha scelto la posizione: 23 (Riga 4, Col 3)\n",
            "Griglia attuale:\n",
            "[[ 7  0  0  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 0  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 11\n",
            "\n",
            "Step 14\n",
            "Dado uscito: 3\n",
            "L'agente ha scelto la posizione: 15 (Riga 3, Col 0)\n",
            "Griglia attuale:\n",
            "[[ 7  0  0  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 11\n",
            "\n",
            "Step 15\n",
            "Dado uscito: 5\n",
            "L'agente ha scelto la posizione: 2 (Riga 0, Col 2)\n",
            "Griglia attuale:\n",
            "[[ 7  0  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  0  7 10  0]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 14\n",
            "\n",
            "Step 16\n",
            "Dado uscito: 9\n",
            "L'agente ha scelto la posizione: 11 (Riga 2, Col 1)\n",
            "Griglia attuale:\n",
            "[[ 7  0  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  0]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 14\n",
            "\n",
            "Step 17\n",
            "Dado uscito: 3\n",
            "L'agente ha scelto la posizione: 14 (Riga 2, Col 4)\n",
            "Griglia attuale:\n",
            "[[ 7  0  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 14\n",
            "\n",
            "Step 18\n",
            "Dado uscito: 7\n",
            "L'agente ha scelto la posizione: 1 (Riga 0, Col 1)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  0]]\n",
            "Punteggio attuale: 19\n",
            "\n",
            "Step 19\n",
            "Dado uscito: 7\n",
            "L'agente ha scelto la posizione: 24 (Riga 4, Col 4)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  0  4  0  0]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 24\n",
            "\n",
            "Step 20\n",
            "Dado uscito: 6\n",
            "L'agente ha scelto la posizione: 16 (Riga 3, Col 1)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  7  4  0  0]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 34\n",
            "\n",
            "Step 21\n",
            "Dado uscito: 8\n",
            "L'agente ha scelto la posizione: 18 (Riga 3, Col 3)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  0]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  7  4  6  0]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 44\n",
            "\n",
            "Step 22\n",
            "Dado uscito: 6\n",
            "L'agente ha scelto la posizione: 9 (Riga 1, Col 4)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  8]\n",
            " [ 0  5  7 10  9]\n",
            " [ 5  7  4  6  0]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 46\n",
            "\n",
            "Step 23\n",
            "Dado uscito: 6\n",
            "L'agente ha scelto la posizione: 10 (Riga 2, Col 0)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  8]\n",
            " [ 6  5  7 10  9]\n",
            " [ 5  7  4  6  0]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 46\n",
            "\n",
            "Step 24\n",
            "Dado uscito: 6\n",
            "L'agente ha scelto la posizione: 19 (Riga 3, Col 4)\n",
            "Griglia attuale:\n",
            "[[ 7  3  3  3  7]\n",
            " [ 8  6  5  8  8]\n",
            " [ 6  5  7 10  9]\n",
            " [ 5  7  4  6  6]\n",
            " [ 8  2  3  5  7]]\n",
            "Punteggio attuale: 47\n",
            "Punteggio Medio: 47.0\n"
          ]
        }
      ],
      "source": [
        "NUM_TEST_EPISODES = 1   # Number of episodes to test the agent\n",
        "MAX_STEPS_TEST = 25    # Maximum steps per test episode\n",
        "if SAVE_CHKPT:\n",
        "    agent.net_eval.load_state_dict(torch.load(CHECKPOINT_NAME))\n",
        "test_knister_agent(env, agent,NUM_TEST_EPISODES, MAX_STEPS_TEST)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "D89qK7MeA-HZ",
        "co4yNAfVnTDY",
        "UOPMjmqfrYGU",
        "v1NvtN_Nn0I9",
        "5AhCz6aHtCNv",
        "KlpI3BfEnOYy",
        "oy2jTQX6yGsA",
        "zXkpgnLo8LrM",
        "NwwzI-Vw3sj2",
        "qHEA1Z-KrF3q",
        "uqbVDy_2sZgU",
        "xlyHHWUTzIS9",
        "c8vCUG7M0nWW",
        "doI-tTPZ07nG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lunar_landing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
