{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DLFUl1bqyp-A"
      },
      "outputs": [],
      "source": [
        "# Standard\n",
        "from collections import namedtuple, deque\n",
        "import random\n",
        "\n",
        "# Third-party\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import trange\n",
        "import gc\n",
        "# knister api\n",
        "from api import KnisterGame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Tui5TWb0s_tw"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Q network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sjLdnOl3s-oh"
      },
      "outputs": [],
      "source": [
        "class QNet(nn.Module):\n",
        "    # Policy Network\n",
        "    def __init__(self, n_state_vars, n_actions, dim_hidden=256):\n",
        "        super(QNet, self).__init__()\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(n_state_vars, dim_hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(dim_hidden, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passes the input through the network layers to output Q-values\n",
        "        return self.fc(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Replay Buffer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xvVh-ABA8g-q"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, n_actions, memory_size, batch_size):\n",
        "        # Initialize actions, batch and experience template\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "        # Initialize the memory\n",
        "        self.memory = deque(maxlen=memory_size)  \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        # Store experience in memory\n",
        "        e = self.experience(state, action, reward, next_state, done)\n",
        "        self.memory.append(e)\n",
        "\n",
        "    def sample(self):\n",
        "        # Sample a batch of experiences\n",
        "        experiences = random.sample(self.memory, k=self.batch_size)  \n",
        "\n",
        "        # Convert to tensors for training\n",
        "        states = torch.from_numpy(\n",
        "            np.vstack([e.state for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        actions = torch.from_numpy(\n",
        "            np.vstack([e.action for e in experiences if e is not None])\n",
        "        ).long().to(device)\n",
        "\n",
        "        rewards = torch.from_numpy(\n",
        "            np.vstack([e.reward for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        next_states = torch.from_numpy(\n",
        "            np.vstack([e.next_state for e in experiences if e is not None])\n",
        "        ).float().to(device)\n",
        "\n",
        "        dones = torch.from_numpy(\n",
        "            np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)\n",
        "        ).float().to(device)\n",
        "\n",
        "        # Return the tuple with all tensors\n",
        "        return (states,actions,rewards,next_states,dones)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DdlsVjtx27Wt"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "    def __init__(\n",
        "        self, n_states, n_actions, batch_size=64, learning_rate=1e-4,\n",
        "        learn_step=5, gamma=0.99, mem_size=int(1e5), tau=1e-3\n",
        "    ):\n",
        "        # Core parameters for learning and updating the Q-network\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.batch_size = batch_size\n",
        "        self.gamma = gamma  # Discount factor for future rewards\n",
        "        self.learn_step = learn_step  # Frequency of learning steps\n",
        "        self.tau = tau  # Rate for soft updating the target network\n",
        "\n",
        "        # Initialize the policy network (net_eval) and target network (net_target)\n",
        "        self.net_eval = QNet(n_states,n_actions).to(device)  \n",
        "        self.net_target = QNet(n_states,n_actions).to(device)  \n",
        "        self.optimizer = optim.Adam(self.net_eval.parameters(),lr=learning_rate) \n",
        "        self.criterion =  nn.MSELoss() \n",
        "\n",
        "        # Initialize memory for experience replay\n",
        "        self.memory = ReplayBuffer(n_actions,mem_size,batch_size)  \n",
        "        self.counter = 0  # Tracks learning steps for periodic updates\n",
        "\n",
        "    def getAction(self, state,available_actions, epsilon):\n",
        "        # Select action using an epsilon-greedy strategy to balance exploration and exploitation\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "        self.net_eval.eval()  # Set network to evaluation mode\n",
        "        with torch.no_grad():\n",
        "            action_values = self.net_eval(state)\n",
        "        self.net_eval.train()  # Return to training mode\n",
        "\n",
        "        # Choose random action with probability epsilon, otherwise choose best predicted action\n",
        "        if random.random() < epsilon:\n",
        "            action = random.choice(available_actions)\n",
        "        else:\n",
        "            # Calculation of q_values\n",
        "            q_values = action_values.cpu().data.numpy().squeeze()\n",
        "            # create a mask with -infinity\n",
        "            mask = np.full(q_values.shape, -1e5)\n",
        "            # coping only the valid actions in the mask\n",
        "            mask[available_actions] = q_values[available_actions]\n",
        "            # Argmax on the mask will always choose a valid action\n",
        "            action = np.argmax(mask)\n",
        "        \n",
        "        return action\n",
        "\n",
        "    def save2Memory(self, state, action, reward, next_state, done):\n",
        "        # Save experience to memory and, if ready, sample from memory and\n",
        "        #  update the network\n",
        "        self.memory.add(state, action, reward, next_state, done)\n",
        "        self.counter += 1  # Increment step counter\n",
        "\n",
        "        # Perform learning every 'learn_step' steps if enough experiences are in memory\n",
        "        if (self.counter % self.learn_step == 0 and len(self.memory) >= self.batch_size): \n",
        "            experiences= self.memory.sample() \n",
        "            self.learn(experiences)\n",
        "\n",
        "    def learn(self, experiences):\n",
        "        # Perform a learning step by minimizing the difference between\n",
        "        #  predicted and target Q-values\n",
        "        states, actions, rewards, next_states, dones = experiences\n",
        "        \n",
        "        ## ADDING MASK FOR INVALID ACTIONS ##\n",
        "        q_target_predictions = self.net_target(next_states).detach()\n",
        "        # retrieve the grids of the next states\n",
        "        next_states_grids = next_states[:, :25]\n",
        "        # Create the mask for valid actions\n",
        "        invalid_mask = (next_states_grids != 0)\n",
        "        # Set Q-values of invalid actions to -inf\n",
        "        q_target_predictions[invalid_mask] = -1e5\n",
        "\n",
        "        # Compute target Q-values from net_target for stability in training\n",
        "        q_target = q_target_predictions.max(1)[0].unsqueeze(1)\n",
        "       \n",
        "        y_j = rewards + (self.gamma * q_target * (1 - dones))\n",
        "            # Bellman equation for target Q-value\n",
        "        q_eval = self.net_eval(states).gather(1, actions)\n",
        "            # Q-value predictions from policy network\n",
        "\n",
        "        # Compute loss and backpropagate to update net_eval\n",
        "        loss = self.criterion(q_eval, y_j)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        # Update target network with soft update for smooth learning\n",
        "        self.targetUpdate()\n",
        "\n",
        "    def targetUpdate(self):\n",
        "        # Soft update to gradually shift target network parameters toward\n",
        "        #  policy network parameters\n",
        "        params = zip(self.net_eval.parameters(), self.net_target.parameters())\n",
        "        for eval_param, target_param in params:\n",
        "            target_param.data.copy_(\n",
        "                self.tau  * eval_param.data + (1.0-self.tau) * target_param.data # Hint: use the `self.tau` rate for soft updating\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Function that calculate the enviroment state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_obs_state(env):\n",
        "    # --- 1. NORMALIZED GRID ---\n",
        "    # 25 values float between 0.0 and 1.0 (or 0 and 12/12)\n",
        "    grid = env.get_grid().flatten() / 12.0\n",
        "    \n",
        "    # --- 2. DICE (One-Hot Encoded) ---\n",
        "    # The dice ranges from 2 to 12 (11 possible values)\n",
        "    dice_val = int(env.get_current_roll())\n",
        "    dice_one_hot = np.zeros(11, dtype=np.float32)\n",
        "    \n",
        "    # If 2 comes out -> index 0, If 12 comes out -> index 10\n",
        "    if 2 <= dice_val <= 12:\n",
        "        dice_one_hot[dice_val - 2] = 1.0\n",
        "        \n",
        "    # --- 3. CONCATENATION ---\n",
        "    # 25 (grid) + 11 (dice) = 36 total inputs\n",
        "    obs = np.concatenate([grid, dice_one_hot])\n",
        "    \n",
        "    return obs.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhRfNjag7fgy"
      },
      "outputs": [],
      "source": [
        "CHECKPOINT_NAME = 'checkpoint.pth'\n",
        "RECENT_EPISODES = 100  # Number of episodes for average score in early stopping\n",
        "MIN_EPISODES_FOR_STOP = 100  # Ensures enough episodes before evaluating target\n",
        "\n",
        "def train(\n",
        "        env, agent, n_episodes, max_steps,\n",
        "        eps_start, eps_end, eps_decay,\n",
        "        target_score, do_store_checkpoint\n",
        "):\n",
        "    # Initialize score history and epsilon (exploration rate)\n",
        "    game_score_hist = []\n",
        "    epsilon_hist = []\n",
        "    epsilon = eps_start\n",
        "\n",
        "    # Progress bar format for tracking training progress\n",
        "    bar_format = '{l_bar}{bar:10}| {n:4}/{total_fmt}'\\\n",
        "                 ' [{elapsed:>7}<{remaining:>7}, {rate_fmt}{postfix}]'\n",
        "    pbar = trange(n_episodes, unit=\"ep\", bar_format=bar_format, ascii=True)\n",
        "\n",
        "    for idx_epi in pbar:\n",
        "        # Reset the environment for a new episode\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)\n",
        "        score = 0.0\n",
        "\n",
        "        for idx_step in range(max_steps):\n",
        "            # Select an action based on the current policy (epsilon-greedy)\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon)  \n",
        "            \n",
        "            # Execute the chosen action in the environment            \n",
        "            env.choose_action(action)\n",
        "\n",
        "            #change of state\n",
        "            next_state = get_obs_state(env)\n",
        "            \n",
        "            #Total reward\n",
        "            reward = env.get_last_reward() \n",
        "            \n",
        "            # Check if the episode is finished\n",
        "            done = env.has_finished()\n",
        "            \n",
        "            # Store experience in memory and update the agent\n",
        "            agent.save2Memory(state,action,reward,next_state,done)  \n",
        "            \n",
        "            # Move to the next state\n",
        "            state = next_state  \n",
        "            \n",
        "            score += reward \n",
        "            # Check if the episode is finished\n",
        "            if done:\n",
        "                break\n",
        "        \n",
        "        # Track scores and decay epsilon for less exploration over time\n",
        "        # Saves the local score of each episode to see it in the graph\n",
        "        game_score = env.get_total_reward()\n",
        "        game_score_hist.append(game_score)\n",
        "        epsilon_hist.append(epsilon)\n",
        "        score_avg = np.mean(game_score_hist[-RECENT_EPISODES:])\n",
        "        epsilon =max(epsilon * eps_decay,eps_end)\n",
        "\n",
        "        # Update the progress bar with the current score and average\n",
        "        pbar.set_postfix_str(\n",
        "            f\"Score: {score: 7.2f}, 100 score avg: {score_avg: 7.2f}, Eps: {epsilon: .4f}\"\n",
        "        )\n",
        "        pbar.update(0)\n",
        "\n",
        "        # Clear GPU memory periodically to prevent memory issues\n",
        "        if idx_epi % 1000 == 0:\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n",
        "            \n",
        "        # Early stopping condition if target score is achieved\n",
        "        if len(game_score_hist) >= 100 and score_avg >= target_score:\n",
        "            print(\"\\nTarget Reached!\")\n",
        "            break\n",
        "\n",
        "    # Print completion message based on early stopping or max episodes\n",
        "    if (idx_epi + 1) < n_episodes:\n",
        "        print(\"\\nTraining complete - target reached!\")\n",
        "    else:\n",
        "        print(\"\\nTraining complete - maximum episodes reached.\")\n",
        "\n",
        "    # Save the trained model if specified\n",
        "    if do_store_checkpoint:\n",
        "        torch.save(agent.net_eval.state_dict(), CHECKPOINT_NAME)\n",
        "\n",
        "    return game_score_hist, epsilon_hist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "lw3c6cHKzInm"
      },
      "outputs": [],
      "source": [
        "def plotScore(scores):\n",
        "    # Plot the agent's score history to visualize learning progress\n",
        "    plt.figure()\n",
        "    plt.plot(scores)\n",
        "    plt.title(\"Score History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotEpsilon(epsilons):\n",
        "    # Plot the agent's epsilon history to visualize exploration rate changes\n",
        "    plt.figure()\n",
        "    plt.plot(epsilons)\n",
        "    plt.title(\"Epsilon History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plotAll(scores, epsilons):\n",
        "    # Plot  score, and epsilon histories in a single figure with subplots\n",
        "    plt.figure(figsize=(12, 8))\n",
        "\n",
        "    plt.subplot(2, 1, 1)\n",
        "    plt.plot(scores)\n",
        "    plt.title(\"Score History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Score\")\n",
        "\n",
        "    plt.subplot(2, 1, 2)\n",
        "    plt.plot(epsilons)\n",
        "    plt.title(\"Epsilon History\")\n",
        "    plt.xlabel(\"Episodes\")\n",
        "    plt.ylabel(\"Epsilon\")\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8vCUG7M0nWW"
      },
      "source": [
        "## **Section 4 - Time to learn (training)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tm7eqwwA0n9M"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64         # Number of experiences sampled per learning step\n",
        "LR = 5e-4                # Learning rate for optimizer\n",
        "EPISODES = 500000        # Maximum number of episodes to train (~1 or 2 minute for 100 episodes)\n",
        "TARGET_SCORE = 65        # Early stop if average score reaches this value\n",
        "GAMMA = 0.99             # Discount factor for future rewards\n",
        "MEMORY_SIZE = 100000      # Maximum capacity of replay memory\n",
        "LEARN_STEP = 10          # Frequency (in steps) of learning updates\n",
        "TAU = 1e-3               # Soft update rate for the target network\n",
        "SAVE_CHKPT = True       # Option to save trained model checkpoint\n",
        "#Exploration parameters\n",
        "MAX_STEPS = 25           # Maximum steps per episode\n",
        "EPS_START = 1             # Initial epsilon for exploration (100% exploration at start)\n",
        "EPS_END = 0.20            # Minimum epsilon (final exploration rate)\n",
        "# Epsilon decay rate (controls exploration reduction)\n",
        "EPS_DECAY = 0.99995 # after 80000 it goes to EPS_END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.Create the Knister game environment\n",
        "\n",
        "env = KnisterGame()\n",
        "\n",
        "# 2. Define state and action dimensions\n",
        "# Input: 25 squares + 11 dice = 36\n",
        "n_states = 36 \n",
        "# Output: 25 possible positions to write\n",
        "n_actions = 25 \n",
        "\n",
        "# 3. Initialize the Agent\n",
        "agent = DQN(\n",
        "    n_states=n_states,\n",
        "    n_actions=n_actions,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    learning_rate=LR, \n",
        "    mem_size=MEMORY_SIZE,\n",
        "    gamma=GAMMA,\n",
        "    learn_step=LEARN_STEP,\n",
        "    tau=TAU\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IWfHJf-A4Gqx"
      },
      "outputs": [],
      "source": [
        "# 4. Start the Training (passing the correct linear decay)\n",
        "game_score_hist,epsilon_hist = train(env, agent, n_episodes=EPISODES, max_steps=MAX_STEPS,\n",
        "                   eps_start=EPS_START, eps_end=EPS_END, eps_decay=EPS_DECAY,\n",
        "                   target_score=TARGET_SCORE, do_store_checkpoint=SAVE_CHKPT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plot of the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5. Plot the results\n",
        "plotAll(game_score_hist, epsilon_hist)\n",
        "# Free up GPU memory if using CUDA\n",
        "if str(device) == \"cuda\":\n",
        "    torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fast test to see the average done by the agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_knister_agent_fast(env, agent,num_tests, max_steps):\n",
        "    print(\"--- INIZIO PARTITA DI TEST ---\")\n",
        "    \n",
        "    step = 1\n",
        "    average_score = []\n",
        "    for i in range(num_tests):\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)  \n",
        "        for step in range(max_steps):#Episode steps\n",
        "            # No casual moves only what it has learned\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon=0.0)\n",
        "            # Execute the move\n",
        "            env.choose_action(action)\n",
        "            # Change state\n",
        "            state = get_obs_state(env)\n",
        "            # Check if game is finished\n",
        "            done = env.has_finished()\n",
        "            step += 1\n",
        "            if done:\n",
        "                break    \n",
        "        average_score.append(env.get_total_reward())\n",
        "        #print(f\"Final Score: {env.get_total_reward()}\")\n",
        "    print(f\"Average Score: {sum(average_score) / len(average_score) if average_score else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- INIZIO PARTITA DI TEST ---\n",
            "Average Score: 42.74\n"
          ]
        }
      ],
      "source": [
        "NUM_TEST_EPISODES = 500   # Number of episodes to test the agent\n",
        "MAX_STEPS_TEST = 25    # Maximum steps per test episode\n",
        "if SAVE_CHKPT:\n",
        "    agent.net_eval.load_state_dict(torch.load(CHECKPOINT_NAME))\n",
        "test_knister_agent_fast(env, agent,NUM_TEST_EPISODES, MAX_STEPS_TEST)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Test con passi per vedere cosa fa il modello"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_knister_agent(env, agent,num_tests, max_steps):\n",
        "    print(\"--- INIZIO PARTITA DI TEST ---\")\n",
        "    \n",
        "    step = 1\n",
        "    average_score = []\n",
        "    for i in range(num_tests):\n",
        "        env.new_game()\n",
        "        state = get_obs_state(env)  \n",
        "        for step in range(max_steps):#episode steps\n",
        "            # No casual moves only what it has learned\n",
        "            action = agent.getAction(state,env.get_available_actions(), epsilon=0.0)\n",
        "            # Execute the move\n",
        "            env.choose_action(action)\n",
        "            # Change state\n",
        "            state = get_obs_state(env)\n",
        "            # Check if game is finished\n",
        "            done = env.has_finished()\n",
        "            \n",
        "            # Retrieve info from the real game for display\n",
        "            current_grid = env.get_grid()\n",
        "            dice_value = env.get_current_roll()\n",
        "            score = env.get_total_reward()\n",
        "            \n",
        "            print(f\"\\nStep {step}\")\n",
        "            print(f\"Dice rolled: {dice_value}\")\n",
        "            print(f\"The agent chose position: {action} (Row {action//5}, Col {action%5})\")\n",
        "            print(\"Current grid:\")\n",
        "            print(current_grid)\n",
        "            print(f\"Current score: {score}\")\n",
        "            \n",
        "            step += 1\n",
        "            if done:\n",
        "                break    \n",
        "        average_score.append(env.get_total_reward())\n",
        "        #print(f\"Final Score: {env.get_total_reward()}\")\n",
        "    print(f\"Average Score: {sum(average_score) / len(average_score) if average_score else 0}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- INIZIO PARTITA DI TEST ---\n",
            "\n",
            "Step 0\n",
            "Dice rolled: 7\n",
            "The agent chose position: 4 (Row 0, Col 4)\n",
            "Current grid:\n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "Current score: 0\n",
            "\n",
            "Step 1\n",
            "Dice rolled: 4\n",
            "The agent chose position: 12 (Row 2, Col 2)\n",
            "Current grid:\n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [0 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "Current score: 0\n",
            "\n",
            "Step 2\n",
            "Dice rolled: 5\n",
            "The agent chose position: 15 (Row 3, Col 0)\n",
            "Current grid:\n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 0 0]\n",
            " [0 0 0 0 0]]\n",
            "Current score: 0\n",
            "\n",
            "Step 3\n",
            "Dice rolled: 7\n",
            "The agent chose position: 23 (Row 4, Col 3)\n",
            "Current grid:\n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 0 0]\n",
            " [0 0 0 5 0]]\n",
            "Current score: 0\n",
            "\n",
            "Step 4\n",
            "Dice rolled: 7\n",
            "The agent chose position: 20 (Row 4, Col 0)\n",
            "Current grid:\n",
            "[[0 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 0 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 2\n",
            "\n",
            "Step 5\n",
            "Dice rolled: 8\n",
            "The agent chose position: 0 (Row 0, Col 0)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 0 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 5\n",
            "\n",
            "Step 6\n",
            "Dice rolled: 6\n",
            "The agent chose position: 18 (Row 3, Col 3)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 0 0 0 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 8 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 5\n",
            "\n",
            "Step 7\n",
            "Dice rolled: 8\n",
            "The agent chose position: 8 (Row 1, Col 3)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 0 0 6 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 8 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 9\n",
            "\n",
            "Step 8\n",
            "Dice rolled: 6\n",
            "The agent chose position: 6 (Row 1, Col 1)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 8 0 6 0]\n",
            " [0 0 7 0 0]\n",
            " [4 0 0 8 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 13\n",
            "\n",
            "Step 9\n",
            "Dice rolled: 8\n",
            "The agent chose position: 16 (Row 3, Col 1)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 8 0 6 0]\n",
            " [0 0 7 0 0]\n",
            " [4 6 0 8 0]\n",
            " [7 0 0 5 0]]\n",
            "Current score: 23\n",
            "\n",
            "Step 10\n",
            "Dice rolled: 5\n",
            "The agent chose position: 24 (Row 4, Col 4)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 8 0 6 0]\n",
            " [0 0 7 0 0]\n",
            " [4 6 0 8 0]\n",
            " [7 0 0 5 8]]\n",
            "Current score: 33\n",
            "\n",
            "Step 11\n",
            "Dice rolled: 11\n",
            "The agent chose position: 13 (Row 2, Col 3)\n",
            "Current grid:\n",
            "[[7 0 0 0 6]\n",
            " [0 8 0 6 0]\n",
            " [0 0 7 5 0]\n",
            " [4 6 0 8 0]\n",
            " [7 0 0 5 8]]\n",
            "Current score: 34\n",
            "\n",
            "Step 12\n",
            "Dice rolled: 5\n",
            "The agent chose position: 7 (Row 1, Col 2)\n",
            "Current grid:\n",
            "[[ 7  0  0  0  6]\n",
            " [ 0  8 11  6  0]\n",
            " [ 0  0  7  5  0]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  0  5  8]]\n",
            "Current score: 34\n",
            "\n",
            "Step 13\n",
            "Dice rolled: 10\n",
            "The agent chose position: 14 (Row 2, Col 4)\n",
            "Current grid:\n",
            "[[ 7  0  0  0  6]\n",
            " [ 0  8 11  6  0]\n",
            " [ 0  0  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  0  5  8]]\n",
            "Current score: 35\n",
            "\n",
            "Step 14\n",
            "Dice rolled: 5\n",
            "The agent chose position: 9 (Row 1, Col 4)\n",
            "Current grid:\n",
            "[[ 7  0  0  0  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 0  0  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  0  5  8]]\n",
            "Current score: 35\n",
            "\n",
            "Step 15\n",
            "Dice rolled: 8\n",
            "The agent chose position: 11 (Row 2, Col 1)\n",
            "Current grid:\n",
            "[[ 7  0  0  0  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 0  5  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  0  5  8]]\n",
            "Current score: 37\n",
            "\n",
            "Step 16\n",
            "Dice rolled: 9\n",
            "The agent chose position: 3 (Row 0, Col 3)\n",
            "Current grid:\n",
            "[[ 7  0  0  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 0  5  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  0  5  8]]\n",
            "Current score: 39\n",
            "\n",
            "Step 17\n",
            "Dice rolled: 7\n",
            "The agent chose position: 22 (Row 4, Col 2)\n",
            "Current grid:\n",
            "[[ 7  0  0  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 0  5  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  9  5  8]]\n",
            "Current score: 39\n",
            "\n",
            "Step 18\n",
            "Dice rolled: 6\n",
            "The agent chose position: 10 (Row 2, Col 0)\n",
            "Current grid:\n",
            "[[ 7  0  0  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  0  8  0]\n",
            " [ 7  0  9  5  8]]\n",
            "Current score: 46\n",
            "\n",
            "Step 19\n",
            "Dice rolled: 5\n",
            "The agent chose position: 19 (Row 3, Col 4)\n",
            "Current grid:\n",
            "[[ 7  0  0  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  0  8  6]\n",
            " [ 7  0  9  5  8]]\n",
            "Current score: 48\n",
            "\n",
            "Step 20\n",
            "Dice rolled: 6\n",
            "The agent chose position: 21 (Row 4, Col 1)\n",
            "Current grid:\n",
            "[[ 7  0  0  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  0  8  6]\n",
            " [ 7  5  9  5  8]]\n",
            "Current score: 50\n",
            "\n",
            "Step 21\n",
            "Dice rolled: 7\n",
            "The agent chose position: 2 (Row 0, Col 2)\n",
            "Current grid:\n",
            "[[ 7  0  6  8  6]\n",
            " [ 0  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  0  8  6]\n",
            " [ 7  5  9  5  8]]\n",
            "Current score: 51\n",
            "\n",
            "Step 22\n",
            "Dice rolled: 9\n",
            "The agent chose position: 5 (Row 1, Col 0)\n",
            "Current grid:\n",
            "[[ 7  0  6  8  6]\n",
            " [ 7  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  0  8  6]\n",
            " [ 7  5  9  5  8]]\n",
            "Current score: 54\n",
            "\n",
            "Step 23\n",
            "Dice rolled: 11\n",
            "The agent chose position: 17 (Row 3, Col 2)\n",
            "Current grid:\n",
            "[[ 7  0  6  8  6]\n",
            " [ 7  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  9  8  6]\n",
            " [ 7  5  9  5  8]]\n",
            "Current score: 55\n",
            "\n",
            "Step 24\n",
            "Dice rolled: 11\n",
            "The agent chose position: 1 (Row 0, Col 1)\n",
            "Current grid:\n",
            "[[ 7 11  6  8  6]\n",
            " [ 7  8 11  6 10]\n",
            " [ 7  5  7  5  5]\n",
            " [ 4  6  9  8  6]\n",
            " [ 7  5  9  5  8]]\n",
            "Current score: 55\n",
            "Average Score: 55.0\n"
          ]
        }
      ],
      "source": [
        "NUM_TEST_EPISODES = 1   # Number of episodes to test the agent\n",
        "MAX_STEPS_TEST = 25    # Maximum steps per test episode\n",
        "if SAVE_CHKPT:\n",
        "    agent.net_eval.load_state_dict(torch.load(CHECKPOINT_NAME))\n",
        "test_knister_agent(env, agent,NUM_TEST_EPISODES, MAX_STEPS_TEST)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "D89qK7MeA-HZ",
        "co4yNAfVnTDY",
        "UOPMjmqfrYGU",
        "v1NvtN_Nn0I9",
        "5AhCz6aHtCNv",
        "KlpI3BfEnOYy",
        "oy2jTQX6yGsA",
        "zXkpgnLo8LrM",
        "NwwzI-Vw3sj2",
        "qHEA1Z-KrF3q",
        "uqbVDy_2sZgU",
        "xlyHHWUTzIS9",
        "c8vCUG7M0nWW",
        "doI-tTPZ07nG"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "lunar_landing",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
